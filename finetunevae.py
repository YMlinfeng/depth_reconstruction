# ===== æ ‡å‡†åº“ =====
import os
import math
import logging
import argparse
import pickle
from pathlib import Path

# ===== ç¬¬ä¸‰æ–¹åº“ =====
import numpy as np
import cv2
import matplotlib.pyplot as plt
import imageio

# ===== PyTorch =====
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision import transforms

# ===== è‡ªå®šä¹‰æ¨¡å— =====
from vqvae.vae_2d_resnet import VAERes2DImg, VAERes2DImgDirectBC, VectorQuantizer, VAERes3DImgDirectBC
from vqganlc.models.vqgan_lc import VQModel
from training.pretrain_dataset import MyDatasetOnlyforVoxel, MyDatasetOnlyforVAE, MyDatasetforVAE3_3, MyDatasetforVAE_FRONT_1_9
from training.train_arg_parser import get_args_parser
from model import LSSTPVDAv2OnlyForVoxel, LSSTPVDAv2
from vismask.gen_vis_mask import sdf2occ
from training import distributed_mode
from training.schedulers import WarmupCosineLRScheduler, SchedulerFactory
from diffusers import AutoencoderKLCogVideoX

logger = logging.getLogger(__name__)


def save_checkpoint(save_path, epoch, vqvae, optimizer, lr_scheduler):
    # save_checkpoint: ç”¨äºå°†å½“å‰æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œ
    # ä»¥å®ç°æ–­ç‚¹ç»­è®­æˆ–äº‹åanalysisã€‚
    # å‡†å¤‡ä¿å­˜å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹å‚æ•°ï¼ˆvqvae.state_dict())ã€ä¼˜åŒ–å™¨å‚æ•°ã€lrè°ƒåº¦å™¨å‚æ•°ã€å½“å‰epochç­‰ä¿¡æ¯
    checkpoint = {
        'epoch': epoch,
        'vqvae_state_dict': vqvae.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'lr_scheduler_state_dict': lr_scheduler.state_dict() if lr_scheduler else None,
    }
    torch.save(checkpoint, save_path)
    print(f"=> Saved checkpoint to {save_path}")


def load_checkpoint(ckpt_path, vqvae, optimizer, lr_scheduler):
    """
    åŠ è½½æ¨¡å‹è®­ç»ƒçš„ checkpointï¼ŒåŒ…æ‹¬æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ã€‚

    å‚æ•°:
        ckpt_path (str): checkpoint æ–‡ä»¶è·¯å¾„
        vqvae (nn.Module): å¾…åŠ è½½å‚æ•°çš„æ¨¡å‹å®ä¾‹
        optimizer (Optimizer): ä¼˜åŒ–å™¨å®ä¾‹
        lr_scheduler (Scheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹ï¼ˆå¯ä¸º Noneï¼‰

    è¿”å›:
        int: åŠ è½½åè®­ç»ƒçš„èµ·å§‹ epoch
    """
    if not os.path.isfile(ckpt_path):
        print(f"=> âŒ No checkpoint found at '{ckpt_path}'")
        return 1  # è‹¥æ²¡æœ‰æ‰¾åˆ° checkpoint æ–‡ä»¶ï¼Œåˆ™ä»ç¬¬ 1 epoch å¼€å§‹

    checkpoint = torch.load(ckpt_path, map_location='cpu')
    start_epoch = checkpoint.get('epoch', 0) + 1

    # æ‹¿å‡ºæƒé‡å­—å…¸
    state_dict = checkpoint.get('vqvae_state_dict', checkpoint)

    # è‡ªåŠ¨å¤„ç† 'module.' å‰ç¼€åŒ¹é…é—®é¢˜
    model_keys = list(vqvae.state_dict().keys())
    ckpt_keys = list(state_dict.keys())

    if model_keys[0].startswith("module.") and not ckpt_keys[0].startswith("module."):
        # æ¨¡å‹æ˜¯ DDPï¼Œä½† checkpoint æ˜¯å•å¡ä¿å­˜çš„ â†’ æ·»åŠ  "module."
        new_state_dict = {"module." + k: v for k, v in state_dict.items()}
        print("âœ… åŠ è½½æƒé‡ï¼šæ·»åŠ  'module.' å‰ç¼€")
    elif not model_keys[0].startswith("module.") and ckpt_keys[0].startswith("module."):
        # æ¨¡å‹æ˜¯å•å¡ï¼Œä½† checkpoint æ˜¯ DDP ä¿å­˜çš„ â†’ å»é™¤ "module."
        new_state_dict = {k.replace("module.", "", 1): v for k, v in state_dict.items()}
        print("âœ… åŠ è½½æƒé‡ï¼šå»é™¤ 'module.' å‰ç¼€")
    else:
        # æ¨¡å‹å’Œ checkpoint ä¿æŒä¸€è‡´ â†’ æ— éœ€æ”¹å
        new_state_dict = state_dict
        print("âœ… åŠ è½½æƒé‡ï¼šæ— éœ€ä¿®æ”¹å‰ç¼€")

    # åŠ è½½å‚æ•°
    vqvae.load_state_dict(new_state_dict, strict=True)
    print(f"=> ğŸ§  Loaded model weights from '{ckpt_path}'")

    # åŠ è½½ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€
    if 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("âœ… Optimizer state loaded.")
    else:
        print("âš ï¸ Warning: Optimizer state not found in checkpoint.")

    if lr_scheduler and checkpoint.get('lr_scheduler_state_dict', None) is not None:
        lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
        print("âœ… LR scheduler state loaded.")
    else:
        print("âš ï¸ Warning: LR scheduler state not found or not used.")

    return start_epoch

def visualize_sample(orig_tensor, recon_tensor, output_prefix, fps):
    """
    å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–ï¼š
      - orig_tensor, recon_tensor çš„shapeå‡ä¸º (C, T, H, W)
      - è¾“å‡ºè§†é¢‘ï¼šå¯¹æ¯ä¸€å¸§å°†åŸå§‹ä¸é‡æ„å¸§æ¨ªå‘æ‹¼æ¥åç”Ÿæˆè§†é¢‘æ–‡ä»¶
      - è¾“å‡ºå›¾ç‰‡ï¼šä¿å­˜æ¯ä¸€å¸§ä¸ºå•å¼ å›¾ç‰‡ï¼ŒåŒæ—¶æ‹¼æ¥æˆ3x3çš„æ‹¼å›¾ä¿å­˜
    å‚æ•°ï¼š
      orig_tensor: åŸå§‹è§†é¢‘æ ·æœ¬ tensorï¼ŒèŒƒå›´ä¸º[-1,1]
      recon_tensor: é‡æ„è§†é¢‘æ ·æœ¬ tensorï¼ŒèŒƒå›´ä¸º[-1,1]
      output_prefix: ä¿å­˜æ—¶çš„æ–‡ä»¶åå‰ç¼€ï¼ˆåŒ…æ‹¬è·¯å¾„ï¼‰
      fps: è§†é¢‘å¸§ç‡
    """

    # ç¡®ä¿ output_prefix çš„çˆ¶ç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_prefix)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)

    # æ–­å¼€æ¢¯åº¦ï¼Œè½¬æ¢ tensor ä¸º numpy æ•°ç»„å¹¶è°ƒæ•´é¡ºåºï¼šä» (C, T, H, W) åˆ° (T, H, W, C)
    orig_np = orig_tensor.detach().cpu().permute(1, 2, 3, 0).numpy()  # (T, H, W, C)
    recon_np = recon_tensor.detach().cpu().permute(1, 2, 3, 0).numpy()

    # å°† [-1,1] æ˜ å°„åˆ° [0,255]
    orig_np = np.clip((orig_np + 1) / 2 * 255, 0, 255).astype(np.uint8)
    recon_np = np.clip((recon_np + 1) / 2 * 255, 0, 255).astype(np.uint8)

    # å¯¹æ¯ä¸€å¸§è¿›è¡Œæ¨ªå‘æ‹¼æ¥ï¼šå½¢æˆå¯¹æ¯”å¸§
    composite_frames = [np.concatenate([orig_np[i], recon_np[i]], axis=1) for i in range(orig_np.shape[0])]

    # ----------------------------------
    # 1) ä¿å­˜è§†é¢‘ï¼šç¡®ä¿ä¿å­˜è§†é¢‘çš„ç›®å½•å­˜åœ¨ï¼Œå†å†™å…¥è§†é¢‘æ–‡ä»¶
    # ----------------------------------
    video_path = output_prefix + "_comparison.mp4"
    video_dir = os.path.dirname(video_path)
    if video_dir:
        os.makedirs(video_dir, exist_ok=True)
    writer = imageio.get_writer(video_path, fps=fps, codec='libx264')
    for frame in composite_frames:
        writer.append_data(frame)
    writer.close()
    print(f"Saved comparison video to {video_path}")

    # ----------------------------------
    # 2) ä¿å­˜å„å¸§å›¾ç‰‡ï¼Œå¹¶ç”Ÿæˆ3x3æ‹¼å›¾
    # ----------------------------------
    frames_folder = output_prefix + "_frames"
    os.makedirs(frames_folder, exist_ok=True)
    for i, frame in enumerate(composite_frames):
        frame_path = os.path.join(frames_folder, f"frame_{i:03d}.png")
        imageio.imwrite(frame_path, frame)
    print(f"Saved individual frame images to folder {frames_folder}")

    # è‹¥å¸§æ•°æ­£å¥½ä¸º9ï¼Œåˆ™æ‹¼æ¥æˆ3x3çš„ collage å›¾
    if len(composite_frames) == 9:
        rows = []
        for i in range(3):
            row = np.concatenate(composite_frames[i*3:(i*3+3)], axis=1)
            rows.append(row)
        collage = np.concatenate(rows, axis=0)
        collage_path = output_prefix + "_collage.png"
        collage_dir = os.path.dirname(collage_path)
        if collage_dir:
            os.makedirs(collage_dir, exist_ok=True)
        imageio.imwrite(collage_path, collage)
        print(f"Saved collage image to {collage_path}")
        
# ---------------------------------------------------------
# ä¿®æ”¹åçš„ validate_vae å‡½æ•°ï¼šéªŒè¯æ—¶ä¿å­˜è§†é¢‘å’Œ 9 å¼ å›¾ç‰‡ï¼ˆæ‹¼å›¾å’Œå•å¸§å‡ä¿å­˜ï¼‰
# ---------------------------------------------------------
def validate_vae(args, vae, val_loader, device, epoch, step):
    vae.eval()
    total_loss = 0.0
    total_recon = 0.0
    total_kl = 0.0
    count = 0
    criterion = torch.nn.MSELoss()
    # æ ‡è®°ï¼šæ˜¯å¦å·²ç»å¯¹ç¬¬ä¸€ä¸ª batch è¿›è¡Œå¯è§†åŒ–
    first_batch_visualized = False
    with torch.no_grad():
        for i, imgs in enumerate(val_loader):
            imgs = imgs.to(device, non_blocking=True)
            # ---- å¯¹ imgs åšé›¶å¡«å…… (F.pad) ä»¥ä¿è¯å®½é«˜èƒ½è¢« 8 æ•´é™¤ ----
            B, C, T, H, W = imgs.shape
            pad_h = (8 - H % 8) % 8
            pad_w = (8 - W % 8) % 8
            if pad_h > 0 or pad_w > 0:
                imgs = F.pad(imgs, (0, pad_w, 0, pad_h), mode='constant', value=0)
            # ----------------------------
            encode_out = vae.encode(imgs, return_dict=True)
            posterior = encode_out.latent_dist
            kl_loss = posterior.kl().mean()
            # reparameterize: è¿™é‡Œé‡‡ç”¨éšæœºé‡‡æ ·
            z = posterior.sample()
            decode_out = vae.decode(z, return_dict=True)
            recons = decode_out.sample
            # é’ˆå¯¹å½“å‰ä»»åŠ¡ï¼šå¦‚æœ recons çš„é€šé“æ•°ä¸º1 è€Œ imgs é€šé“ä¸º3ï¼Œåˆ™é‡å¤æ‰©å±•
            if recons.shape[2] == 1 and imgs.shape[1] == 3:
                recons = recons.repeat(1, 1, 3, 1, 1)
            recon_loss = criterion(recons, imgs)
            loss = recon_loss + kl_loss
            total_loss += loss.item()
            total_recon += recon_loss.item()
            total_kl += kl_loss.item()
            count += 1

            # å¯¹ç¬¬ä¸€ä¸ª batch ç¬¬ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
            if not first_batch_visualized:
                # è°ƒç”¨è¾…åŠ©å‡½æ•°è¿›è¡Œå¯è§†åŒ–
                vis_prefix = os.path.join(args.validate_path, f"val_epoch{epoch}_step{step}")
                visualize_sample(imgs[0], recons[0], vis_prefix, args.fps)
                first_batch_visualized = True

            if i >= args.max_val_steps - 1:
                break
    avg_loss = total_loss / count
    avg_recon = total_recon / count
    avg_kl = total_kl / count
    print(f"[Val] Epoch {epoch} Step {step}: Avg Loss: {avg_loss:.6f}, Recon Loss: {avg_recon:.6f}, KL Loss: {avg_kl:.6f}")
    vae.train()


def finetune_vae(args, vae, train_loader, val_loader, device):
    # è‹¥ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œå…ˆå–å‡ºåŸå§‹æ¨¡å‹
    vae = vae.module if isinstance(vae, torch.nn.parallel.DistributedDataParallel) else vae
    optimizer = torch.optim.AdamW(vae.parameters(), lr=args.lr, betas=(0.9, 0.999), weight_decay=0.01)

    total_steps = len(train_loader) * args.epochs
    warmup_steps = 500  # 500 warmup steps

    if args.use_scheduler:
        factory = SchedulerFactory(
            optimizer=optimizer,
            total_steps=total_steps,
            warmup_steps=warmup_steps,
            base_lr=args.lr,
            min_lr=1e-6
        )
        lr_scheduler = factory.get(name="cosine")  # è¿”å› SchedulerWrapper
    else:
        lr_scheduler = None

    # å°è¯•åŠ è½½ checkpointï¼ˆè‹¥ args.resume ä¸º Trueï¼Œä¸”åœ¨ args.resume_ckpt ä¸­æŒ‡å®šäº† ckpt æ–‡ä»¶è·¯å¾„ï¼‰
    start_epoch = 1
    if args.resume:
        start_epoch = load_checkpoint(args.resume_ckpt, vae, optimizer, lr_scheduler)

    vae.train()

    # åˆå§‹åŒ–æ··åˆç²¾åº¦çš„ GradScaler
    scaler = torch.cuda.amp.GradScaler()

    # å®šä¹‰æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆè¯·åœ¨è¿è¡Œæ—¶é€šè¿‡ args ä¼ å…¥è¯¥å‚æ•°ï¼Œæ¯”å¦‚ --gradient_accumulation_steps è®¾ä¸º 4ï¼‰
    accumulation_steps = args.gradient_accumulation_steps

    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, args.epochs + 1):
        # è‹¥ä½¿ç”¨äº†åˆ†å¸ƒå¼Samplerï¼Œéœ€è¦è®¾ç½®å½“å‰epochï¼Œä»¥ä¾¿å®ƒåšå¥½shuffle
        if hasattr(train_loader.sampler, 'set_epoch'):
            train_loader.sampler.set_epoch(epoch)

        # æ¸…ç©ºæ¢¯åº¦ï¼ˆå¼€å§‹æ–°çš„æ¢¯åº¦ç´¯ç§¯å‘¨æœŸï¼‰
        optimizer.zero_grad()

        for step, imgs in enumerate(train_loader, start=1):
            imgs = imgs.to(device, non_blocking=True)

            # =====================
            # 1) 3DVAEå‰å‘ä¼ æ’­ï¼ˆæ·»åŠ é›¶å¡«å……ä¿è¯å®½é«˜èƒ½è¢«8æ•´é™¤ï¼‰
            B, C, T, H, W = imgs.shape  # ä¾‹å¦‚ï¼štorch.Size([B, 3, 9, 300, 450])
            pad_h = (8 - H % 8) % 8
            pad_w = (8 - W % 8) % 8
            if pad_h > 0 or pad_w > 0:
                imgs = F.pad(imgs, (0, pad_w, 0, pad_h), mode='constant', value=0)

            with torch.cuda.amp.autocast():
                # (1) encode => å¾—åˆ°åéªŒåˆ†å¸ƒ
                encode_out = vae.encode(imgs, return_dict=True) 
                posterior = encode_out.latent_dist
                kl_loss = posterior.kl().mean()  # KL æ•£åº¦æŸå¤±

                # (2) reparameterize (sampling)
                z = posterior.sample() # torch.Size([1, 16, 3, 38, 57])

                # (3) decode => é‡æ„è¾“å‡º
                decode_out = vae.decode(z, return_dict=True)
                recons = decode_out.sample  # ä¾‹å¦‚ï¼š[B, 3, 9, 304, 456]
                # è‹¥é‡æ„è¾“å‡ºé€šé“ä¸åŒ¹é…ï¼ˆä¾‹å¦‚ recons ä¸º1é€šé“ï¼Œè€Œ imgs ä¸º3é€šé“ï¼‰ï¼Œåˆ™æ‰©å±•é€šé“

                # è®¡ç®—é‡æ„æŸå¤±ï¼ˆç®€å•é‡‡ç”¨MSELossï¼‰ä¸æ€» loss
                recon_loss = F.mse_loss(recons, imgs)
                # total_loss = recon_loss + kl_loss
                total_loss = recon_loss

            # -----------------------------
            # 2) æ¢¯åº¦ç´¯ç§¯ï¼šå¯¹ loss è¿›è¡Œç¼©æ”¾ååå‘ä¼ æ’­
            # -----------------------------
            # è¿™é‡Œå°†å½“å‰æ¢¯åº¦è´¡çŒ®å‡æ‘Šåˆ°æ¯ä¸ªç´¯ç§¯æ­¥ä¸­
            loss_to_accumulate = total_loss / accumulation_steps
            scaler.scale(loss_to_accumulate).backward()

            # å¯é€‰ï¼šæ‰“å°æ—¥å¿—
            if step % args.log_interval == 0:
                current_lr = lr_scheduler.get_lr() if lr_scheduler else args.lr
                print(f"[Epoch {epoch}/{args.epochs}] Step {step}/{len(train_loader)} "
                      f"ReconLoss: {recon_loss.item():.6f} "
                      f"KL: {kl_loss.item():.6f} "
                      f"TotalLoss: {total_loss.item():.6f} "
                      f"LR: {current_lr:.6f}")

            # åœ¨ç¬¬ä¸€ä¸ª epoch ç¬¬ä¸€ä¸ª step æ—¶è¿›è¡Œå¯è§†åŒ–ï¼ˆä¿å­˜è§†é¢‘å’Œ9å¼ å›¾ç‰‡ï¼‰
            if epoch == start_epoch and step == 1:
                vis_prefix = os.path.join(args.checkpoint_dir, f"train_epoch{epoch}_step{step}")
                visualize_sample(imgs[0], recons[0], vis_prefix, args.fps)

            # # ä¸­é€”è¿›è¡ŒéªŒè¯
            # if step % args.val_interval == 0 and val_loader is not None:
            #     validate_vae(args, vae, val_loader, device, epoch, step)

            # # å®šæœŸä¿å­˜ checkpoint
            # if step % args.save_interval == 0:
            #     save_path = os.path.join(args.checkpoint_dir, f"vqvae_epoch{epoch}_step{step}.pth")
            #     save_checkpoint(save_path, epoch, vae, optimizer, lr_scheduler)

            # æ¯ accumulation_steps æˆ–å½“å¤„äºå½“å‰ epoch æœ€åä¸€ä¸ª batch æ—¶ï¼Œæ›´æ–°å‚æ•°
            if (step % accumulation_steps == 0) or (step == len(train_loader)):
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                if lr_scheduler:
                    lr_scheduler.step()

        # Epoch ç»“æŸæ—¶å¯é€‰æ‹©ä¿å­˜ checkpoint
        if args.save_end_of_epoch:
            save_path = os.path.join(args.checkpoint_dir, f"vqvae_epoch{epoch}.pth")
            save_checkpoint(save_path, epoch, vae, optimizer, lr_scheduler)
            validate_vae(args, vae, val_loader, device, epoch, step)

    print("Training Finished!")

def main():

    parser = argparse.ArgumentParser("Train VQ-VAE/VAE Model")
    parser.add_argument("--dist_on_itp",
                        action="store_true",
                        default=False,
                        help="ä½¿ç”¨åŸºäº ITp çš„åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆå¦‚ MPI çš„ OMPI ç¯å¢ƒï¼‰ if set.")
    parser.add_argument("--rank",
                        type=int,
                        default=0,
                        help="åˆ†å¸ƒå¼è®­ç»ƒä¸­å½“å‰è¿›ç¨‹çš„å…¨å±€æ’åï¼ˆrankï¼‰.")
    parser.add_argument("--world_size",
                        type=int,
                        default=1,
                        help="å‚ä¸åˆ†å¸ƒå¼è®­ç»ƒçš„æ€»è¿›ç¨‹æ•°.")
    parser.add_argument("--gpu",
                        type=int,
                        default=0,
                        help="å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU id.")
    parser.add_argument("--dist_backend",
                        type=str,
                        default="nccl",
                        help="åˆ†å¸ƒå¼åç«¯ï¼ˆæ¨èä½¿ç”¨ 'nccl'ï¼‰.")
    parser.add_argument("--model", type=str, default="VQModel", help="choices: VAERes2DImgDirectBC, VQModel, Cog3DVAE")
    parser.add_argument("--mode", type=str, default="train", help="choices: train, eval")
    parser.add_argument("--dataset", type=str, default="MyDatasetOnlyforVoxel", help="Dataset name for logging")
    parser.add_argument("--pkl_path", type=str, default="/mnt/bn/occupancy3d/workspace/lzy/robotics-data-sdk/data_infos/pkls/dcr_data_dynamic_bottom_scaleddepth.pkl", help="Dataset name for logging")
    parser.add_argument("--validate_path", type=str, default='./validation/d408_1024', help="12")
    parser.add_argument("--batch_size", type=int, default=16, help="batch size per GPU")
    parser.add_argument("--num_workers", type=int, default=8, help="number of data loading workers")
    parser.add_argument("--pin_mem", action='store_true', help="use pin memory in DataLoader")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints_408", help="Where to save checkpoints")
    parser.add_argument("--resume", action='store_true', help="resume training from ckpt")
    parser.add_argument("--resume_ckpt", type=str, default="", help="which ckpt to resume from")
    parser.add_argument("--lr", type=float, default=1e-4, help="learning rate for VQ-VAE") # åŸç‰ˆ4.5e-6
    parser.add_argument("--epochs", type=int, default=2, help="total epochs to train VQ-VAE")
    parser.add_argument("--save_interval", type=int, default=1000, help="save ckpt every n steps")
    parser.add_argument("--val_interval", type=int, default=2000, help="run validation every n steps")
    parser.add_argument("--log_interval", type=int, default=10, help="print log every n steps")
    parser.add_argument("--save_end_of_epoch", action='store_false', help="save checkpoint at each epoch end") #flag
    parser.add_argument("--use_scheduler", action='store_false', help="use StepLR scheduler or not")
    parser.add_argument("--max_val_steps", type=int, default=5, help="max batch for val step")
    parser.add_argument("--input_height", type=int, default=520,
                        help="å›¾åƒè¾“å…¥é«˜åº¦")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4, help="Number of steps to accumulate gradients.")
    parser.add_argument("--input_width", type=int, default=784,
                        help="å›¾åƒè¾“å…¥å®½åº¦")
    parser.add_argument("--inp_channels", type=int, default=80, help="è¾“å…¥é€šé“æ•°")
    parser.add_argument("--out_channels", type=int, default=80, help="è¾“å‡ºé€šé“æ•°")
    parser.add_argument("--mid_channels", type=int, default=320, help="éšè—å±‚é€šé“æ•°") # 1024
    parser.add_argument("--z_channels", type=int, default=4, help="æ½œå˜é‡é€šé“æ•°") # 256
    parser.add_argument("--fps", type=int, default=2, help="æ½œå˜é‡é€šé“æ•°") 
    parser.add_argument('--concat', action='store_true', help='...')
    # parser.add_argument("--dtype", type=str, default="float16",
    #                     help="The data type for computation (e.g., 'float16' or 'bfloat16')")

    args = parser.parse_args()
    print(args)
    # -------------------------------
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (å¤šæœºå¤šå¡ or å•æœºå¤šå¡)
    # -------------------------------
    distributed_mode.init_distributed_mode(args)
    local_rank = args.gpu
    torch.cuda.set_device(local_rank)
    
    device = torch.device("cuda", local_rank) if torch.cuda.is_available() else torch.device("cpu")
    os.makedirs(args.checkpoint_dir, exist_ok=True)

    dataset_train = MyDatasetforVAE3_3(args) 
    # dataset_train = MyDatasetforVAE_FRONT_1_9(args)
    
    num_tasks = distributed_mode.get_world_size() # 1
    global_rank = distributed_mode.get_rank() # 0
    print(f"--------num_tasks:{num_tasks}, global_rank:{global_rank}-----------")
    
    sampler_train = torch.utils.data.DistributedSampler(
        dataset_train,
        num_replicas=num_tasks,
        rank=global_rank,
        shuffle=True
    )
    train_loader = DataLoader(
        dataset_train,
        sampler=sampler_train,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=args.pin_mem, # å½“è®¾ç½®ä¸º True æ—¶ï¼Œå°†æ•°æ®åŠ è½½åˆ°é”é¡µå†…å­˜ä¸­ï¼Œè¿™æ ·å¯ä»¥åŠ é€Ÿæ•°æ®ä» CPU åˆ° GPU çš„æ‹·è´ï¼Œå°¤å…¶å½“ä½ çš„ batch_size è¾ƒå¤§æ—¶æ•ˆæœæ›´æ˜æ˜¾
        drop_last=True, # å¦‚æœä½ çš„æŸå¤±å‡½æ•°æˆ–æ¨¡å‹å¯¹ batch_size å¾ˆæ•æ„Ÿï¼Œè®¾ç½® drop_last=True å¯ä»¥é¿å…æœ€åä¸€ä¸ª batch å°ºå¯¸ä¸å‡çš„é—®é¢˜
    )
    logger.info("Intializing DataLoader")

    # ----------------------------------------------------------------------------------
    # ç›´æ¥ä»è®­ç»ƒé›†ä¸­å–å‰200æ¡åšval
    # ----------------------------------------------------------------------------------
    small_subset_size = 200
    if len(dataset_train) > small_subset_size:
        dataset_val = torch.utils.data.Subset(dataset_train, range(small_subset_size))  # å–å‰200æ¡åšéªŒè¯
        val_sampler = DistributedSampler(dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=False)
        val_loader = DataLoader(
            dataset_val,
            sampler=val_sampler,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            pin_memory=False,
            drop_last=False,
            # å½“æ•°æ®æ ·æœ¬çš„ç»“æ„æ¯”è¾ƒå¤æ‚æˆ–è€…æ¯ä¸ªæ ·æœ¬å¤§å°ä¸ä¸€è‡´æ—¶ï¼Œå†…ç½®çš„ default_collate å¯èƒ½æ— æ³•æ»¡è¶³éœ€æ±‚
            # collate_fn=my_collate_fn, #! collate_fn çš„èŒè´£æ˜¯å°†å¤šä¸ªæ ·æœ¬ç»„åˆæˆä¸€ä¸ª batch
        )
        logger.info("Initialized a small validation set from training data")
    else:
        val_loader = None


    # todoï¼šåŠ è½½æƒé‡ï¼Œè·¯å¾„ï¼š/mnt/bn/occupancy3d/workspace/mzj/CogVideoX-2b/vae/ è¿™ä¸ªè·¯å¾„ä¸‹æœ‰diffusion_pytorch_model.safetensorså’Œconfig.jsonä¸¤ä¸ªæ–‡ä»¶
    vae = AutoencoderKLCogVideoX.from_pretrained(
        "/mnt/bn/occupancy3d/workspace/mzj/CogVideoX-2b/vae/",
        # torch_dtype= torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    )

    vae.to(device)

    if distributed_mode.get_world_size() > 1:
        vae = DDP(vae, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True) # æ ¹æ®warningä¿®æ­£

    vae_total_params = sum(p.numel() for p in vae.parameters())
    vae_trainable_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)
    print(f"[Model Param Count] VQ-VAE total: {vae_total_params}, trainable: {vae_trainable_params}")
    
    finetune_vae(args, vae, train_loader, val_loader, device)

    print("Done!")


if __name__ == "__main__":
    main()
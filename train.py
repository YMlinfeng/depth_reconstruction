# ===== æ ‡å‡†åº“ =====
import os
import math
import logging
import argparse
import pickle
from pathlib import Path

# ===== ç¬¬ä¸‰æ–¹åº“ =====
import numpy as np
import cv2
import matplotlib.pyplot as plt

# ===== PyTorch =====
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torch.utils.data import Dataset, DataLoader, DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision import transforms

# ===== è‡ªå®šä¹‰æ¨¡å— =====
from vqvae.vae_2d_resnet import VAERes2DImg, VAERes2DImgDirectBC, VectorQuantizer, VAERes3DImgDirectBC
from vqganlc.models.vqgan_lc import VQModel
from training.pretrain_dataset import MyDatasetOnlyforVoxel, MyDatasetOnlyforVAE
from training.train_arg_parser import get_args_parser
from model import LSSTPVDAv2OnlyForVoxel, LSSTPVDAv2
from vismask.gen_vis_mask import sdf2occ
from training import distributed_mode
from training.schedulers import WarmupCosineLRScheduler, SchedulerFactory
from diffusers import AutoencoderKLCogVideoX
from tools.load_config import load_config
from torch.distributed.elastic.multiprocessing.errors import record

# ç¡®ä¿å¼‚å¸¸ä¿¡æ¯èƒ½å¤Ÿå†™å…¥æŒ‡å®šæ–‡ä»¶
if "TORCHELASTIC_ERROR_FILE" not in os.environ:
    # è¿™é‡Œå°†é”™è¯¯ä¿¡æ¯å†™å…¥å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ error_log.json
    os.environ["TORCHELASTIC_ERROR_FILE"] = os.path.join(os.getcwd(), "error_log.json")

logger = logging.getLogger(__name__)


def my_collate_fn(batch):
    """
    batch æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢å¦‚ (imgs_tensor, [img_meta])
    æˆ‘ä»¬å¯¹ imgs_tensor ä½¿ç”¨ torch.stack è¿›è¡Œå †å ï¼Œ
    è€Œå¯¹å…ƒæ•°æ®åˆ™ç›´æ¥æå–ï¼ˆæ³¨æ„è¿™é‡Œå‡è®¾æ¯ä¸ªæ ·æœ¬è¿”å›çš„æ˜¯ä¸€ä¸ªåªæœ‰ä¸€ä¸ª dict çš„ listï¼‰ã€‚
    """
    imgs = torch.stack([item[0] for item in batch], dim=0)
    # ç”±äº __getitem__ è¿”å›çš„æ˜¯ [img_meta]ï¼Œå› æ­¤å–å‡ºåˆ—è¡¨å†…çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
    metas = [item[1][0] for item in batch]
    return imgs, metas


def save_checkpoint(save_path, epoch, vqvae, optimizer, lr_scheduler):
    # save_checkpoint: ç”¨äºå°†å½“å‰æ¨¡å‹å’Œä¼˜åŒ–å™¨çš„çŠ¶æ€ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œ
    # ä»¥å®ç°æ–­ç‚¹ç»­è®­æˆ–äº‹åanalysisã€‚
    # å‡†å¤‡ä¿å­˜å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹å‚æ•°ï¼ˆvqvae.state_dict())ã€ä¼˜åŒ–å™¨å‚æ•°ã€lrè°ƒåº¦å™¨å‚æ•°ã€å½“å‰epochç­‰ä¿¡æ¯
    checkpoint = {
        'epoch': epoch,
        'vqvae_state_dict': vqvae.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'lr_scheduler_state_dict': lr_scheduler.state_dict() if lr_scheduler else None,
    }
    torch.save(checkpoint, save_path)
    print(f"=> Saved checkpoint to {save_path}")

# load_checkpoint: åœ¨éœ€è¦æ–­ç‚¹ç»­è®­æ—¶ï¼Œä»å·²æœ‰çš„ checkpoint æ–‡ä»¶ä¸­æ¢å¤è®­ç»ƒçš„è¿›åº¦ã€æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨å‚æ•°ç­‰ã€‚
def load_checkpoint(ckpt_path, vqvae, optimizer, lr_scheduler):
    """
    åŠ è½½æ¨¡å‹è®­ç»ƒçš„ checkpointï¼ŒåŒ…æ‹¬æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ã€‚

    å‚æ•°:
        ckpt_path (str): checkpoint æ–‡ä»¶è·¯å¾„
        vqvae (nn.Module): å¾…åŠ è½½å‚æ•°çš„æ¨¡å‹å®ä¾‹
        optimizer (Optimizer): ä¼˜åŒ–å™¨å®ä¾‹
        lr_scheduler (Scheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹ï¼ˆå¯ä¸º Noneï¼‰

    è¿”å›:
        int: åŠ è½½åè®­ç»ƒçš„èµ·å§‹ epoch
    """
    if not os.path.isfile(ckpt_path):
        print(f"=> âŒ No checkpoint found at '{ckpt_path}'")
        return 1  # è‹¥æ²¡æœ‰æ‰¾åˆ° checkpoint æ–‡ä»¶ï¼Œåˆ™ä»ç¬¬ 1 epoch å¼€å§‹

    checkpoint = torch.load(ckpt_path, map_location='cpu')
    start_epoch = checkpoint.get('epoch', 0) + 1

    # æ‹¿å‡ºæƒé‡å­—å…¸
    state_dict = checkpoint.get('vqvae_state_dict', checkpoint)

    # è‡ªåŠ¨å¤„ç† 'module.' å‰ç¼€åŒ¹é…é—®é¢˜
    model_keys = list(vqvae.state_dict().keys())
    ckpt_keys = list(state_dict.keys())

    if model_keys[0].startswith("module.") and not ckpt_keys[0].startswith("module."):
        # æ¨¡å‹æ˜¯ DDPï¼Œä½† checkpoint æ˜¯å•å¡ä¿å­˜çš„ â†’ æ·»åŠ  "module."
        new_state_dict = {"module." + k: v for k, v in state_dict.items()}
        print("âœ… åŠ è½½æƒé‡ï¼šæ·»åŠ  'module.' å‰ç¼€")
    elif not model_keys[0].startswith("module.") and ckpt_keys[0].startswith("module."):
        # æ¨¡å‹æ˜¯å•å¡ï¼Œä½† checkpoint æ˜¯ DDP ä¿å­˜çš„ â†’ å»é™¤ "module."
        new_state_dict = {k.replace("module.", "", 1): v for k, v in state_dict.items()}
        print("âœ… åŠ è½½æƒé‡ï¼šå»é™¤ 'module.' å‰ç¼€")
    else:
        # æ¨¡å‹å’Œ checkpoint ä¿æŒä¸€è‡´ â†’ æ— éœ€æ”¹å
        new_state_dict = state_dict
        print("âœ… åŠ è½½æƒé‡ï¼šæ— éœ€ä¿®æ”¹å‰ç¼€")

    # åŠ è½½å‚æ•°
    vqvae.load_state_dict(new_state_dict, strict=True)
    print(f"=> ğŸ§  Loaded model weights from '{ckpt_path}'")

    # åŠ è½½ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€
    if 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("âœ… Optimizer state loaded.")
    else:
        print("âš ï¸ Warning: Optimizer state not found in checkpoint.")

    if lr_scheduler and checkpoint.get('lr_scheduler_state_dict', None) is not None:
        lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
        print("âœ… LR scheduler state loaded.")
    else:
        print("âš ï¸ Warning: LR scheduler state not found or not used.")

    return start_epoch


def train_vqvae(args, model, vqvae, train_loader, val_loader, device):
    optimizer = torch.optim.AdamW(vqvae.parameters(), lr=args.lr, betas=(0.9, 0.999),weight_decay=0.01)

    total_steps = len(train_loader) * args.epochs
    # warmup_steps = int(0.05 * total_steps)  # 5% warmup
    warmup_steps = 500  # 500 warmup

    if args.use_scheduler:
        factory = SchedulerFactory(
            optimizer=optimizer,
            total_steps=total_steps,
            warmup_steps=warmup_steps,
            base_lr=args.lr,
            min_lr=1e-6
        )
        lr_scheduler = factory.get(name="cosine")  # è¿”å› SchedulerWrapper
    else:
        lr_scheduler = None

    # å°è¯•åŠ è½½ checkpointï¼ˆè‹¥ args.resume ä¸º Trueï¼Œä¸”åœ¨ args.resume_ckpt ä¸­æŒ‡å®šäº† ckpt æ–‡ä»¶è·¯å¾„ï¼‰
    start_epoch = 1
    if args.resume:
        start_epoch = load_checkpoint(args.resume_ckpt, vqvae, optimizer, lr_scheduler)
    
    vqvae.train()
    model.eval()

    # ä¸»è®­ç»ƒå¾ªç¯ï¼Œä» start_epoch åˆ° args.epochsï¼Œæ¯ä¸ªepochéå†ä¸€é train_loader
    for epoch in range(start_epoch, args.epochs + 1):
        # è‹¥ä½¿ç”¨äº†åˆ†å¸ƒå¼Samplerï¼Œéœ€è¦è®¾ç½®å½“å‰epochï¼Œä»¥ä¾¿å®ƒåšå¥½shuffle
        if hasattr(train_loader.sampler, 'set_epoch'):
            train_loader.sampler.set_epoch(epoch)
        
        for step, (imgs, img_metas) in enumerate(train_loader, start=1):
            imgs = imgs.to(device, non_blocking=True)       
            with torch.no_grad():
                voxel3 = model(imgs, img_metas)  # voxel[0].shape: [B, 4, 60, 100, 20]
            voxel = voxel3[1]
            # vqvae_out = vqvae(voxel)
            # logits = vqvae_out['logits']
            # print("logits.is_leaf =", logits.is_leaf)
            # logits.retain_grad()  # âœ… æ˜¾å¼ä¿ç•™ä¸­é—´å¼ é‡çš„æ¢¯åº¦
            # # logits.requires_grad_(True)

            # # åªä¿ç•™ä¸€å°æ®µ Decode Path
            # embed_loss = vqvae_out['embed_loss']     # é‡åŒ–æŸå¤±(æ ‡é‡)
            # loss = F.mse_loss(logits, torch.zeros_like(logits)) + embed_loss
    
            # depths, rgbs = model.module.pts_bbox_head.decode_sdf([voxel, logits], img_metas)

            # dloss = F.l1_loss(depths[0], depths[1]) + F.l1_loss(rgbs[0], rgbs[1])
            # loss += dloss
            # loss.backward()

            # print(logits.grad is not None)  # True â†’ è¡¨ç¤º loss èƒ½å›ä¼ åˆ° logitsï¼Œè®¡ç®—å›¾æ˜¯é€šçš„

            # print("++++++++++++++++++")

            vqvae_out = vqvae(voxel)   # åŒ…å« 'logits', 'embed_loss'
            reconstructed_sdf = vqvae_out['logits']  # é‡æ„ç»“æœ [B,4,60,100,20]
            reconstructed_sdf.retain_grad()
            embed_loss = vqvae_out['embed_loss']     # é‡åŒ–æŸå¤±(æ ‡é‡)
            quant = vqvae_out['quant']
            input_tensor = vqvae_out['input']
            quant.retain_grad()

            core = model.module if hasattr(model, 'module') else model
            depths, rgbs = core.pts_bbox_head.decode_sdf([input_tensor, reconstructed_sdf], img_metas)
            recon_loss = F.l1_loss(depths[0], depths[1]) + F.l1_loss(rgbs[0], rgbs[1])
          
            # åŠ ä¸€ä¸ª dummy lossï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å—è¾“å‡ºéƒ½å‚ä¸ loss
            dummy = 0.0
            for name, param in vqvae.named_parameters():
                # if 'gpt' in name:
                dummy = dummy + 0.0 * (param ** 2).sum()

            total_loss = recon_loss + embed_loss + dummy  
            
            optimizer.zero_grad()
            total_loss.backward()


            if step == 1:
                # from torchviz import make_dot
                # make_dot(total_loss, params=dict(vqvae.named_parameters())).render("graph", format="pdf")
                # print("âœ… è®¡ç®—å›¾ç”Ÿæˆå®Œæˆï¼Œä¿å­˜åœ¨å½“å‰ç›®å½•çš„ graph.pdf")
                no_grad_params = []
                for name, param in vqvae.named_parameters():
                    if param.requires_grad and param.grad is None:
                        no_grad_params.append(name)

                if len(no_grad_params) == 0:
                    print("âœ… All parameters have gradients.")
                else:
                    for name in no_grad_params:
                        print(f"[No Grad] {name}")

            optimizer.step()
            if lr_scheduler:
                lr_scheduler.step()
                current_lr = lr_scheduler.get_lr()
            else:
                current_lr = args.lr
                
            if step % args.log_interval == 0:
                print(f"[Epoch {epoch}/{args.epochs}] Step {step}/{len(train_loader)} "
                      f"ReconLoss: {recon_loss.item():.8f}, EmbedLoss: {embed_loss.item():.4f}, TotalLoss: {total_loss.item():.4f}, "
                      f"Current LR: {current_lr:.6f}")
            
            # # ä¸­é€”ä¹Ÿå¯ä»¥åšä¸€æ¬¡ç®€å•éªŒè¯
            # if step % args.val_interval == 0 and val_loader is not None:
            #     if args.general_mode == "vqgan":
            #         validate_vqvae(args, vqvae, val_loader, model, device, epoch, step)
            #     else:
            #         validate_vae(args, vqvae, val_loader, device, epoch, step)
            
            if step % args.save_interval == 0:
                save_path = os.path.join(args.checkpoint_dir, f"{args.general_mode}_epoch{epoch}_step{step}.pth")
                save_checkpoint(save_path, epoch, vqvae, optimizer, lr_scheduler)

        if args.save_end_of_epoch:
            save_path = os.path.join(args.checkpoint_dir, f"{args.general_mode}_epoch{epoch}.pth")
            save_checkpoint(save_path, epoch, vqvae, optimizer, lr_scheduler)
    
    print("Training Finished!")

@record
def main():

    parser = argparse.ArgumentParser("Train VQ-VAE/VAE Model")
    # ---------------------------
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    # ---------------------------
    parser.add_argument("--dist_on_itp",
                        action="store_true",
                        default=False,
                        help="ä½¿ç”¨åŸºäº ITp çš„åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆå¦‚ MPI çš„ OMPI ç¯å¢ƒï¼‰ if set.")
    parser.add_argument("--rank",
                        type=int,
                        default=0,
                        help="åˆ†å¸ƒå¼è®­ç»ƒä¸­å½“å‰è¿›ç¨‹çš„å…¨å±€æ’åï¼ˆrankï¼‰.")
    parser.add_argument("--world_size",
                        type=int,
                        default=1,
                        help="å‚ä¸åˆ†å¸ƒå¼è®­ç»ƒçš„æ€»è¿›ç¨‹æ•°.")
    parser.add_argument("--gpu",
                        type=int,
                        default=0,
                        help="å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU id.")
    parser.add_argument("--dist_backend",
                        type=str,
                        default="nccl",
                        help="åˆ†å¸ƒå¼åç«¯ï¼ˆæ¨èä½¿ç”¨ 'nccl'ï¼‰.")
    parser.add_argument("--model", type=str, default="VQModel", help="choices: VAERes2DImgDirectBC, VQModel, Cog3DVAE")
    parser.add_argument("--mode", type=str, default="train", help="choices: train, eval")
    parser.add_argument("--general_mode", type=str, default="vae", help="choices: vae, vqgan")
    parser.add_argument("--dataset", type=str, default="MyDatasetOnlyforVoxel", help="Dataset name for logging")
    parser.add_argument("--validate_path", type=str, default='./output/d408_1024', help="12")
    parser.add_argument("--batch_size", type=int, default=16, help="batch size per GPU")
    parser.add_argument("--num_workers", type=int, default=8, help="number of data loading workers")
    parser.add_argument("--pin_mem", action='store_true', help="use pin memory in DataLoader")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints_408", help="Where to save checkpoints")
    parser.add_argument("--resume", action='store_true', help="resume training from ckpt")
    parser.add_argument("--resume_ckpt", type=str, default="", help="which ckpt to resume from")
    parser.add_argument("--lr", type=float, default=1e-4, help="learning rate for VQ-VAE") # åŸç‰ˆ4.5e-6
    parser.add_argument("--epochs", type=int, default=2, help="total epochs to train VQ-VAE")
    parser.add_argument("--save_interval", type=int, default=1000, help="save ckpt every n steps")
    parser.add_argument("--val_interval", type=int, default=2000, help="run validation every n steps")
    parser.add_argument("--log_interval", type=int, default=10, help="print log every n steps")
    parser.add_argument("--save_end_of_epoch", action='store_false', help="save checkpoint at each epoch end") #flag
    parser.add_argument("--use_scheduler", action='store_false', help="use StepLR scheduler or not")
    parser.add_argument("--max_val_steps", type=int, default=5, help="max batch for val step")
    parser.add_argument("--input_height", type=int, default=518,help="å›¾åƒè¾“å…¥é«˜åº¦")
    parser.add_argument("--input_width", type=int, default=784,help="å›¾åƒè¾“å…¥å®½åº¦")
    parser.add_argument("--inp_channels", type=int, default=80, help="è¾“å…¥é€šé“æ•°")
    parser.add_argument("--out_channels", type=int, default=80, help="è¾“å‡ºé€šé“æ•°")
    parser.add_argument("--mid_channels", type=int, default=320, help="éšè—å±‚é€šé“æ•°") # 1024
    parser.add_argument("--z_channels", type=int, default=4, help="æ½œå˜é‡é€šé“æ•°") # 256
    parser.add_argument("--img_shape", type=lambda s: tuple(map(int, s.split(','))), default="80,60,100,1", help="å›¾åƒæ•°æ®å½¢çŠ¶")
    parser.add_argument("--encoder_type", type=str, default="vqgan", help="Encoderç±»å‹") # å¯é€‰vqgan or vqgan_lc   
    parser.add_argument("--vq_config_path", type=str, default="/mnt/bn/occupancy3d/workspace/mzj/mp_pretrain/vqganlc/vqgan_configs/vqganlc_16384.yaml", help="Encoderç±»å‹")    
    parser.add_argument("--tuning_codebook", type=int, default=-1, help="Frozen or Tuning Coebook")
    parser.add_argument("--use_cblinear", type=int, default=2, help="Using Projector") # 1æ˜¯Linearï¼Œ2æ˜¯MLP
    parser.add_argument("--quantizer_type", type=str, default="default", help="Quantizerç±»å‹") # é EMA æƒ…å†µ
    # parser.add_argument("--local_embedding_path", default="cluster_codebook_1000cls_100000.pth")
    parser.add_argument("--n_vision_words", type=int, default=16384, help="Codebook Size")
    parser.add_argument("--e_dim", type=int, default=1600, help="Codebook Size")
    parser.add_argument('--concat', action='store_true', help='...')

    args = parser.parse_args()
    print(args)
    # -------------------------------
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (å¤šæœºå¤šå¡ or å•æœºå¤šå¡)
    # -------------------------------
    distributed_mode.init_distributed_mode(args)
    local_rank = args.gpu
    torch.cuda.set_device(local_rank)
    
    device = torch.device("cuda", local_rank) if torch.cuda.is_available() else torch.device("cpu")
    os.makedirs(args.checkpoint_dir, exist_ok=True)

    # =======================
    # 1) æ•°æ®åŠ è½½
    # =======================
    logger.info(f"Initializing Dataset: {args.dataset}")
    if args.general_mode == "vae":
        dataset_train = MyDatasetOnlyforVAE(args) 
    elif args.general_mode == "vqgan":
        dataset_train = MyDatasetOnlyforVoxel(args)
    # è·å–æ€»è¿›ç¨‹(å¡)æ•°
    num_tasks = distributed_mode.get_world_size() # 1
    # è·å–å½“å‰è¿›ç¨‹ï¼ˆå¡ï¼‰ç¼–å·
    global_rank = distributed_mode.get_rank() # 0
    print(f"--------num_tasks:{num_tasks}, global_rank:{global_rank}-----------")
    
    sampler_train = torch.utils.data.DistributedSampler(
        dataset_train,
        num_replicas=num_tasks,
        rank=global_rank,
        shuffle=True
    )
    train_loader = DataLoader(
        dataset_train,
        sampler=sampler_train,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=args.pin_mem,
        drop_last=True,
        collate_fn=my_collate_fn,
    )
    logger.info("Intializing DataLoader")

    # ----------------------------------------------------------------------------------
    # ç›´æ¥ä»è®­ç»ƒé›†ä¸­å–å‰200æ¡åšval
    # ----------------------------------------------------------------------------------
    small_subset_size = 200
    if len(dataset_train) > small_subset_size:
        dataset_val = torch.utils.data.Subset(dataset_train, range(small_subset_size))  # å–å‰200æ¡åšéªŒè¯
        val_sampler = DistributedSampler(dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=False)
        val_loader = DataLoader(
            dataset_val,
            sampler=val_sampler,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            pin_memory=False,
            drop_last=False,
            collate_fn=my_collate_fn,
        )
        logger.info("Initialized a small validation set from training data")

    if args.general_mode == "vqgan":
        model = LSSTPVDAv2OnlyForVoxel(num_classes=4, args=args)
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæ¯•")
        state_dict = torch.load(
            '/mnt/bn/occupancy3d/workspace/lzy/Occ3d/work_dirs/pretrainv0.7_lsstpv_vits_multiextrin_datasetv0.2_rgb/epoch_1.pth',
            map_location='cpu'
        )['state_dict']
        model.load_state_dict(state_dict, strict=False)
        # print(model.load_state_dict(state_dict, strict=False))
        print("åŠ è½½åŸå§‹æƒé‡æ–‡ä»¶å®Œæ¯•") # ä»…å ç”¨1492MBæ˜¾å­˜
        # for name, param in model.named_parameters():
        #     param.requires_grad = False
        # for param in model.parameters():
        #     param.requires_grad = False  # æœ¬æ¨¡å‹ä»…ç”¨äºæ¨ç†ï¼Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°
        model.to(device) 
        model.eval()

    # =======================
    # 3) æ„é€  VQ-VAE (è®­ç»ƒæ¨¡å‹)
    # =======================
    # æ³¨æ„ï¼šå·²ç»æœ‰å®šä¹‰ï¼Œè¿™é‡Œåªéœ€è¦å®ä¾‹åŒ–å³å¯
    vqvae_cfg = None  # å¦‚æœæœ‰é¢å¤–vqvaeçš„é…ç½®ï¼Œå¯åœ¨æ­¤ä¼ 

    if args.model == "VAERes2DImgDirectBC":
        vqvae = VAERes2DImgDirectBC(
            inp_channels=args.inp_channels,
            out_channels=args.out_channels,
            mid_channels=args.mid_channels,
            z_channels=args.z_channels,
            vqvae_cfg=vqvae_cfg
        )
    elif args.model == "VAERes3DImgDirectBC":
        vqvae = VAERes3DImgDirectBC(
            args=args,
            inp_channels=args.inp_channels,
            out_channels=args.out_channels,
            mid_channels=args.mid_channels,
            z_channels=args.z_channels,
            vqvae_cfg=vqvae_cfg
        )
    elif args.model == "VQModel":
        config = load_config(args.vq_config_path, display=True)
        vqvae = VQModel(args=args, ddconfig=config.model.params.ddconfig)
    elif args.model == "Cog3DVAE":
        vqvae = AutoencoderKLCogVideoX(
            in_channels=args.inp_channels,
            out_channels=args.out_channels,
            sample_height=args.input_height,
            sample_width=args.input_width,
            latent_channels=4,
            temporal_compression_ratio=4.0,
        )
    else:
        raise ValueError("æœªè¯†åˆ«çš„æ¨¡å‹ç±»å‹: " + args.model)
    
    vqvae.to(device)
    print("è®­ç»ƒçš„æ¨¡å‹ç±»å‹ä¸º:", args.model)
    # -------------------------------------------------------------------
    # ä»¥ä¸‹ä¸ºå¤šæœºå¤šå¡æ”¯æŒï¼šè‹¥world_size>1ï¼Œå°±ç”¨DDPå°è£… model å’Œ vqvae
    # æ³¨æ„ï¼šLSSTPVDAv2OnlyForVoxel åªæ¨ç†ï¼Œä¸è®­ç»ƒï¼›è‹¥ä»æƒ³è®©å…¶å¤šå¡å¹¶è¡Œä»¥åˆ†æ‘Šæ˜¾å­˜ï¼Œä¹Ÿå¯DDP
    # -------------------------------------------------------------------
    if distributed_mode.get_world_size() > 1:
        if args.general_mode == "vqgan":
            model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)
        vqvae = DDP(vqvae, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False) # æ ¹æ®warningä¿®æ­£

    vqvae_total_params = sum(p.numel() for p in vqvae.parameters())
    vqvae_trainable_params = sum(p.numel() for p in vqvae.parameters() if p.requires_grad)
    # print(f"[Model Param Count] LSSTPVDAv2OnlyForVoxel total: {model_total_params}, trainable: {model_trainable_params}")
    print(f"[Model Param Count] VQ-VAE total: {vqvae_total_params}, trainable: {vqvae_trainable_params}")
    
    try:
        train_vqvae(args, model, vqvae, train_loader, val_loader, device)
    except Exception as e:
        # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œè¿™é‡Œå¯ä»¥æ‰“å°æ—¥å¿—ï¼Œ@record ä¼šè‡ªåŠ¨æ•è·å¼‚å¸¸å¹¶å°†è¯¦ç»†é”™è¯¯ä¿¡æ¯å†™å…¥ TORCHELASTIC_ERROR_FILE æŒ‡å®šçš„æ–‡ä»¶ä¸­
        print("è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸:", e)
        raise

    print("Done!")


if __name__ == "__main__":
    main()